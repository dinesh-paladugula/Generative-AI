## Neural Networks

**Neural Networks (NNs)** are a key part of today's AI and machine learning. They are built to work a bit like the human brain. Imagine many small 
computer parts, called "neurons," connected in layers. These networks learn to find patterns and make guesses by changing how strongly these connections work when they are trained with data. A basic neural network has an input layer (where data goes in), one or more hidden layers (where the magic happens), and an output layer (where the answer comes out). Some common types are Feedforward Neural Networks, Convolutional Neural Networks (CNNs) for images, and Recurrent Neural Networks (RNNs) for sequences.

## Deep Learning

**Deep Learning** is a special part of machine learning that uses very large neural networks, called "deep" neural networks because they have many layers. This allows them to learn very complex things from huge amounts of data. Deep learning is great for tasks like recognizing faces in pictures or understanding spoken words. It's the technology behind many cool AI things we see today, like self-driving cars and smart assistants. Think of it this way: AI is the big field, machine learning is a part of AI, and deep learning is a part of machine learning that uses these deep neural networks.

## Tokens

In the world of language AI, like when computers understand and create text, **tokens** are the small pieces of text that the computer works with. When a computer reads a sentence, it first breaks it down into these tokens. A token can be a whole word, part of a word, or even a single letter or punctuation mark. For example, the sentence "I love learning" might be broken into "I", "love", and "learning". How a computer breaks text into tokens is important because it affects how well the computer can understand and use language.

## Propagation

**Propagation** is how information moves through a neural network. There are two main ways this happens:

*   **Forward Propagation:** This is when the computer takes new information and pushes it through the network, from the first layer all the way to the last layer, to get an answer or prediction. Each neuron does its job, passing its result to the next one.
*   **Backward Propagation (Backpropagation):** This is how neural networks learn. After the network makes a guess, it checks how wrong its guess was. Then, it sends this "error" information backward through the network. This helps the network figure out which connections (weights and biases) need to be adjusted to make better guesses next time.

## Transformers

**Transformers** are a very important type of neural network that has changed how computers deal with language. They are special because they use something called "self-attention" to figure out how important different words are in a sentence. This helps them understand long sentences much better than older types of networks. Transformers are now used in many powerful language AI systems, like the ones that power chatbots and translation tools.

## Parallelism

**Parallelism** means doing many calculations at the same time. In machine learning and deep learning, this is super important because training big AI models needs a lot of computing power and takes a long time. By using parallelism, we can split up the work and have many computers or parts of a computer work on different pieces of the problem at the same time. This makes training much faster. There are different ways to do this, like splitting the data among many processors (data parallelism) or splitting the model itself (model parallelism). Parallelism is a big reason why AI models are getting more and more powerful.
